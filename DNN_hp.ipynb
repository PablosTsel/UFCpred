{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN with Hyper-Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3476, 42) | X_test shape: (1159, 42) | y_train shape: (3476,) | y_test shape: (1159,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "\n",
    "#hyper-parameter tuning\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\" # folder names as a timestamp\n",
    "\n",
    "UFC_FINAL = pd.read_csv(\"UFC_FINAL.csv\")\n",
    "\n",
    "# encode blue and red as 1 and 0\n",
    "UFC_FINAL[\"Winner\"] = UFC_FINAL[\"Winner\"].replace(\"Blue\", 1)\n",
    "UFC_FINAL[\"Winner\"] = UFC_FINAL[\"Winner\"].replace(\"Red\", 0)\n",
    "\n",
    "# train/test split\n",
    "X = UFC_FINAL.drop(\"Winner\", axis=1).values\n",
    "y = UFC_FINAL[\"Winner\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=101)\n",
    "\n",
    "# scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape} | X_test shape: {X_test.shape} | y_train shape: {y_train.shape} | y_test shape: {y_test.shape}\")\n",
    "\n",
    "# function to build the model (argument: hyper-parameter)\n",
    "def build_model(hp):\n",
    "    # model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first layer's no. of neurons = hp.Int range of values to test\n",
    "    model.add(Dense(hp.Int(\"input_units\", min_value=30,max_value=256,step=16), activation='relu'))\n",
    "    \n",
    "    # range of 1 to 10 layers to test\n",
    "    for i in range(hp.Int(\"no. Of Hidden Layers\", 1, 10)):\n",
    "        # for each layer, again test range of neurons\n",
    "        model.add(Dense(hp.Int(f\"Hidden_layer_{i+1}_units\", min_value=30,max_value=256,step=16), activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "        build_model, # name of the function that builds the model\n",
    "        objective=\"val_acc\", # the thing that we're interested to trace\n",
    "        max_trials = 2, # no. of combinations to try\n",
    "        executions_per_trial = 1, # no. of times to train each combination (true avg)\n",
    "        directory=LOG_DIR) # directory to save the outputs\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping # prevent diverge of loss & val_loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3476 samples, validate on 1159 samples\n",
      "Epoch 1/500\n",
      "3476/3476 [==============================] - ETA: 1:41 - loss: 0.6977 - acc: 0.312 - ETA: 7s - loss: 0.6855 - acc: 0.6298  - ETA: 3s - loss: 0.6653 - acc: 0.676 - ETA: 2s - loss: 0.6512 - acc: 0.681 - ETA: 1s - loss: 0.6505 - acc: 0.676 - ETA: 0s - loss: 0.6480 - acc: 0.673 - ETA: 0s - loss: 0.6441 - acc: 0.677 - ETA: 0s - loss: 0.6398 - acc: 0.679 - ETA: 0s - loss: 0.6374 - acc: 0.680 - ETA: 0s - loss: 0.6379 - acc: 0.677 - 2s 583us/sample - loss: 0.6382 - acc: 0.6761 - val_loss: 0.6315 - val_acc: 0.6644\n",
      "Epoch 2/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5584 - acc: 0.781 - ETA: 0s - loss: 0.6098 - acc: 0.691 - ETA: 0s - loss: 0.6121 - acc: 0.689 - ETA: 0s - loss: 0.6121 - acc: 0.687 - ETA: 0s - loss: 0.6071 - acc: 0.692 - ETA: 0s - loss: 0.6115 - acc: 0.685 - ETA: 0s - loss: 0.6057 - acc: 0.689 - ETA: 0s - loss: 0.6089 - acc: 0.686 - ETA: 0s - loss: 0.6135 - acc: 0.681 - ETA: 0s - loss: 0.6142 - acc: 0.680 - ETA: 0s - loss: 0.6145 - acc: 0.682 - ETA: 0s - loss: 0.6125 - acc: 0.683 - ETA: 0s - loss: 0.6143 - acc: 0.680 - 1s 220us/sample - loss: 0.6133 - acc: 0.6824 - val_loss: 0.6169 - val_acc: 0.6644\n",
      "Epoch 3/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.7642 - acc: 0.500 - ETA: 0s - loss: 0.6272 - acc: 0.670 - ETA: 0s - loss: 0.6053 - acc: 0.687 - ETA: 0s - loss: 0.5955 - acc: 0.688 - ETA: 0s - loss: 0.6001 - acc: 0.684 - ETA: 0s - loss: 0.5990 - acc: 0.685 - ETA: 0s - loss: 0.6015 - acc: 0.682 - ETA: 0s - loss: 0.6070 - acc: 0.673 - ETA: 0s - loss: 0.6033 - acc: 0.681 - ETA: 0s - loss: 0.5994 - acc: 0.686 - ETA: 0s - loss: 0.6003 - acc: 0.684 - ETA: 0s - loss: 0.6024 - acc: 0.683 - 1s 200us/sample - loss: 0.6028 - acc: 0.6824 - val_loss: 0.6133 - val_acc: 0.6644\n",
      "Epoch 4/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6325 - acc: 0.687 - ETA: 0s - loss: 0.6067 - acc: 0.680 - ETA: 0s - loss: 0.5999 - acc: 0.685 - ETA: 0s - loss: 0.5987 - acc: 0.685 - ETA: 0s - loss: 0.6058 - acc: 0.675 - ETA: 0s - loss: 0.6056 - acc: 0.672 - ETA: 0s - loss: 0.6010 - acc: 0.680 - ETA: 0s - loss: 0.5995 - acc: 0.683 - ETA: 0s - loss: 0.5978 - acc: 0.687 - ETA: 0s - loss: 0.5998 - acc: 0.687 - ETA: 0s - loss: 0.5967 - acc: 0.691 - ETA: 0s - loss: 0.5969 - acc: 0.691 - 1s 230us/sample - loss: 0.5955 - acc: 0.6919 - val_loss: 0.6105 - val_acc: 0.6652\n",
      "Epoch 5/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5201 - acc: 0.750 - ETA: 0s - loss: 0.5567 - acc: 0.710 - ETA: 0s - loss: 0.5793 - acc: 0.689 - ETA: 0s - loss: 0.5804 - acc: 0.696 - ETA: 0s - loss: 0.5766 - acc: 0.708 - ETA: 0s - loss: 0.5802 - acc: 0.701 - ETA: 0s - loss: 0.5882 - acc: 0.691 - ETA: 0s - loss: 0.5924 - acc: 0.692 - ETA: 0s - loss: 0.5904 - acc: 0.694 - ETA: 0s - loss: 0.5947 - acc: 0.689 - ETA: 0s - loss: 0.5969 - acc: 0.685 - ETA: 0s - loss: 0.5924 - acc: 0.691 - 1s 199us/sample - loss: 0.5910 - acc: 0.6922 - val_loss: 0.6187 - val_acc: 0.6635\n",
      "Epoch 6/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6634 - acc: 0.656 - ETA: 0s - loss: 0.5546 - acc: 0.730 - ETA: 0s - loss: 0.5618 - acc: 0.720 - ETA: 0s - loss: 0.5783 - acc: 0.699 - ETA: 0s - loss: 0.5836 - acc: 0.695 - ETA: 0s - loss: 0.5833 - acc: 0.699 - ETA: 0s - loss: 0.5806 - acc: 0.701 - ETA: 0s - loss: 0.5776 - acc: 0.702 - ETA: 0s - loss: 0.5782 - acc: 0.703 - ETA: 0s - loss: 0.5819 - acc: 0.699 - ETA: 0s - loss: 0.5840 - acc: 0.696 - ETA: 0s - loss: 0.5882 - acc: 0.692 - ETA: 0s - loss: 0.5897 - acc: 0.691 - 1s 207us/sample - loss: 0.5890 - acc: 0.6919 - val_loss: 0.6045 - val_acc: 0.6652\n",
      "Epoch 7/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.7088 - acc: 0.593 - ETA: 0s - loss: 0.6013 - acc: 0.656 - ETA: 0s - loss: 0.6049 - acc: 0.663 - ETA: 0s - loss: 0.5837 - acc: 0.687 - ETA: 0s - loss: 0.5867 - acc: 0.687 - ETA: 0s - loss: 0.5839 - acc: 0.687 - ETA: 0s - loss: 0.5841 - acc: 0.687 - ETA: 0s - loss: 0.5755 - acc: 0.695 - ETA: 0s - loss: 0.5799 - acc: 0.695 - ETA: 0s - loss: 0.5809 - acc: 0.696 - ETA: 0s - loss: 0.5844 - acc: 0.695 - 1s 193us/sample - loss: 0.5840 - acc: 0.6965 - val_loss: 0.5985 - val_acc: 0.6833\n",
      "Epoch 8/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6043 - acc: 0.687 - ETA: 0s - loss: 0.5888 - acc: 0.703 - ETA: 0s - loss: 0.5732 - acc: 0.705 - ETA: 0s - loss: 0.5800 - acc: 0.695 - ETA: 0s - loss: 0.5871 - acc: 0.692 - ETA: 0s - loss: 0.5883 - acc: 0.695 - ETA: 0s - loss: 0.5817 - acc: 0.700 - ETA: 0s - loss: 0.5757 - acc: 0.704 - ETA: 0s - loss: 0.5749 - acc: 0.707 - ETA: 0s - loss: 0.5739 - acc: 0.706 - ETA: 0s - loss: 0.5766 - acc: 0.701 - ETA: 0s - loss: 0.5811 - acc: 0.694 - 1s 197us/sample - loss: 0.5826 - acc: 0.6945 - val_loss: 0.5977 - val_acc: 0.6790\n",
      "Epoch 9/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6081 - acc: 0.687 - ETA: 0s - loss: 0.5748 - acc: 0.714 - ETA: 0s - loss: 0.5821 - acc: 0.704 - ETA: 0s - loss: 0.5853 - acc: 0.701 - ETA: 0s - loss: 0.5812 - acc: 0.703 - ETA: 0s - loss: 0.5805 - acc: 0.701 - ETA: 0s - loss: 0.5769 - acc: 0.702 - ETA: 0s - loss: 0.5741 - acc: 0.702 - ETA: 0s - loss: 0.5751 - acc: 0.705 - ETA: 0s - loss: 0.5811 - acc: 0.699 - ETA: 0s - loss: 0.5854 - acc: 0.696 - ETA: 0s - loss: 0.5827 - acc: 0.698 - 1s 190us/sample - loss: 0.5822 - acc: 0.6991 - val_loss: 0.6058 - val_acc: 0.6730\n",
      "Epoch 10/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5178 - acc: 0.687 - ETA: 0s - loss: 0.5495 - acc: 0.710 - ETA: 0s - loss: 0.5615 - acc: 0.703 - ETA: 0s - loss: 0.5655 - acc: 0.711 - ETA: 0s - loss: 0.5694 - acc: 0.713 - ETA: 0s - loss: 0.5704 - acc: 0.711 - ETA: 0s - loss: 0.5753 - acc: 0.705 - ETA: 0s - loss: 0.5771 - acc: 0.704 - ETA: 0s - loss: 0.5755 - acc: 0.706 - ETA: 0s - loss: 0.5787 - acc: 0.700 - ETA: 0s - loss: 0.5755 - acc: 0.706 - 1s 190us/sample - loss: 0.5762 - acc: 0.7080 - val_loss: 0.6002 - val_acc: 0.6885\n",
      "Epoch 11/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5350 - acc: 0.718 - ETA: 0s - loss: 0.5808 - acc: 0.690 - ETA: 0s - loss: 0.5668 - acc: 0.703 - ETA: 0s - loss: 0.5679 - acc: 0.709 - ETA: 0s - loss: 0.5731 - acc: 0.707 - ETA: 0s - loss: 0.5718 - acc: 0.712 - ETA: 0s - loss: 0.5710 - acc: 0.711 - ETA: 0s - loss: 0.5738 - acc: 0.706 - ETA: 0s - loss: 0.5799 - acc: 0.700 - ETA: 0s - loss: 0.5747 - acc: 0.705 - ETA: 0s - loss: 0.5760 - acc: 0.703 - 1s 174us/sample - loss: 0.5758 - acc: 0.7040 - val_loss: 0.5958 - val_acc: 0.6808\n",
      "Epoch 12/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5345 - acc: 0.718 - ETA: 0s - loss: 0.5495 - acc: 0.753 - ETA: 0s - loss: 0.5744 - acc: 0.722 - ETA: 0s - loss: 0.5689 - acc: 0.717 - ETA: 0s - loss: 0.5712 - acc: 0.713 - ETA: 0s - loss: 0.5796 - acc: 0.708 - ETA: 0s - loss: 0.5765 - acc: 0.706 - ETA: 0s - loss: 0.5725 - acc: 0.706 - ETA: 0s - loss: 0.5770 - acc: 0.706 - ETA: 0s - loss: 0.5793 - acc: 0.703 - ETA: 0s - loss: 0.5769 - acc: 0.706 - 1s 179us/sample - loss: 0.5763 - acc: 0.7068 - val_loss: 0.6060 - val_acc: 0.6747\n",
      "Epoch 13/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5178 - acc: 0.718 - ETA: 0s - loss: 0.5849 - acc: 0.706 - ETA: 0s - loss: 0.5959 - acc: 0.678 - ETA: 0s - loss: 0.6004 - acc: 0.663 - ETA: 0s - loss: 0.5929 - acc: 0.673 - ETA: 0s - loss: 0.5840 - acc: 0.688 - ETA: 0s - loss: 0.5814 - acc: 0.693 - ETA: 0s - loss: 0.5779 - acc: 0.699 - ETA: 0s - loss: 0.5772 - acc: 0.703 - ETA: 0s - loss: 0.5744 - acc: 0.704 - ETA: 0s - loss: 0.5733 - acc: 0.707 - 1s 176us/sample - loss: 0.5731 - acc: 0.7071 - val_loss: 0.6011 - val_acc: 0.6661\n",
      "Epoch 14/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5198 - acc: 0.750 - ETA: 0s - loss: 0.5470 - acc: 0.725 - ETA: 0s - loss: 0.5762 - acc: 0.720 - ETA: 0s - loss: 0.5743 - acc: 0.718 - ETA: 0s - loss: 0.5573 - acc: 0.728 - ETA: 0s - loss: 0.5615 - acc: 0.727 - ETA: 0s - loss: 0.5607 - acc: 0.728 - ETA: 0s - loss: 0.5614 - acc: 0.729 - ETA: 0s - loss: 0.5641 - acc: 0.727 - ETA: 0s - loss: 0.5678 - acc: 0.723 - 1s 164us/sample - loss: 0.5665 - acc: 0.7238 - val_loss: 0.6011 - val_acc: 0.6747\n",
      "Epoch 15/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5959 - acc: 0.687 - ETA: 0s - loss: 0.5731 - acc: 0.713 - ETA: 0s - loss: 0.5826 - acc: 0.702 - ETA: 0s - loss: 0.5690 - acc: 0.707 - ETA: 0s - loss: 0.5638 - acc: 0.709 - ETA: 0s - loss: 0.5617 - acc: 0.714 - ETA: 0s - loss: 0.5599 - acc: 0.717 - ETA: 0s - loss: 0.5608 - acc: 0.716 - ETA: 0s - loss: 0.5590 - acc: 0.718 - ETA: 0s - loss: 0.5610 - acc: 0.718 - ETA: 0s - loss: 0.5645 - acc: 0.716 - 1s 185us/sample - loss: 0.5644 - acc: 0.7152 - val_loss: 0.6054 - val_acc: 0.6721\n",
      "Epoch 16/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.5483 - acc: 0.781 - ETA: 0s - loss: 0.5296 - acc: 0.777 - ETA: 0s - loss: 0.5441 - acc: 0.756 - ETA: 0s - loss: 0.5494 - acc: 0.741 - ETA: 0s - loss: 0.5480 - acc: 0.745 - ETA: 0s - loss: 0.5532 - acc: 0.735 - ETA: 0s - loss: 0.5499 - acc: 0.740 - ETA: 0s - loss: 0.5570 - acc: 0.734 - ETA: 0s - loss: 0.5603 - acc: 0.729 - ETA: 0s - loss: 0.5538 - acc: 0.734 - ETA: 0s - loss: 0.5536 - acc: 0.733 - ETA: 0s - loss: 0.5554 - acc: 0.729 - ETA: 0s - loss: 0.5581 - acc: 0.725 - ETA: 0s - loss: 0.5607 - acc: 0.722 - ETA: 0s - loss: 0.5627 - acc: 0.720 - ETA: 0s - loss: 0.5636 - acc: 0.719 - 1s 269us/sample - loss: 0.5620 - acc: 0.7195 - val_loss: 0.6014 - val_acc: 0.6713\n",
      "Epoch 17/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5274 - acc: 0.843 - ETA: 0s - loss: 0.5646 - acc: 0.743 - ETA: 0s - loss: 0.5534 - acc: 0.735 - ETA: 0s - loss: 0.5562 - acc: 0.728 - ETA: 0s - loss: 0.5632 - acc: 0.719 - ETA: 0s - loss: 0.5665 - acc: 0.713 - ETA: 0s - loss: 0.5545 - acc: 0.724 - ETA: 0s - loss: 0.5577 - acc: 0.724 - ETA: 0s - loss: 0.5589 - acc: 0.722 - ETA: 0s - loss: 0.5551 - acc: 0.725 - ETA: 0s - loss: 0.5528 - acc: 0.726 - 1s 177us/sample - loss: 0.5540 - acc: 0.7250 - val_loss: 0.6092 - val_acc: 0.6678\n",
      "Epoch 18/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.4946 - acc: 0.843 - ETA: 0s - loss: 0.5682 - acc: 0.697 - ETA: 0s - loss: 0.5588 - acc: 0.725 - ETA: 0s - loss: 0.5393 - acc: 0.741 - ETA: 0s - loss: 0.5474 - acc: 0.734 - ETA: 0s - loss: 0.5533 - acc: 0.730 - ETA: 0s - loss: 0.5539 - acc: 0.730 - ETA: 0s - loss: 0.5610 - acc: 0.723 - ETA: 0s - loss: 0.5641 - acc: 0.718 - ETA: 0s - loss: 0.5656 - acc: 0.718 - ETA: 0s - loss: 0.5591 - acc: 0.724 - ETA: 0s - loss: 0.5574 - acc: 0.724 - ETA: 0s - loss: 0.5618 - acc: 0.719 - 1s 216us/sample - loss: 0.5607 - acc: 0.7221 - val_loss: 0.6289 - val_acc: 0.6644\n",
      "Epoch 19/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5095 - acc: 0.687 - ETA: 0s - loss: 0.5971 - acc: 0.704 - ETA: 0s - loss: 0.5745 - acc: 0.720 - ETA: 0s - loss: 0.5657 - acc: 0.716 - ETA: 0s - loss: 0.5592 - acc: 0.724 - ETA: 0s - loss: 0.5575 - acc: 0.718 - ETA: 0s - loss: 0.5706 - acc: 0.709 - ETA: 0s - loss: 0.5723 - acc: 0.709 - ETA: 0s - loss: 0.5682 - acc: 0.712 - ETA: 0s - loss: 0.5643 - acc: 0.715 - ETA: 0s - loss: 0.5630 - acc: 0.717 - ETA: 0s - loss: 0.5616 - acc: 0.716 - ETA: 0s - loss: 0.5592 - acc: 0.718 - 1s 211us/sample - loss: 0.5590 - acc: 0.7186 - val_loss: 0.6064 - val_acc: 0.6756\n",
      "Epoch 20/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.4815 - acc: 0.718 - ETA: 0s - loss: 0.5576 - acc: 0.683 - ETA: 0s - loss: 0.5557 - acc: 0.700 - ETA: 0s - loss: 0.5453 - acc: 0.718 - ETA: 0s - loss: 0.5465 - acc: 0.720 - ETA: 0s - loss: 0.5508 - acc: 0.717 - ETA: 0s - loss: 0.5446 - acc: 0.722 - ETA: 0s - loss: 0.5441 - acc: 0.725 - ETA: 0s - loss: 0.5442 - acc: 0.726 - ETA: 0s - loss: 0.5410 - acc: 0.729 - ETA: 0s - loss: 0.5449 - acc: 0.727 - ETA: 0s - loss: 0.5464 - acc: 0.727 - ETA: 0s - loss: 0.5521 - acc: 0.724 - 1s 212us/sample - loss: 0.5504 - acc: 0.7273 - val_loss: 0.6089 - val_acc: 0.6678\n",
      "Epoch 21/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6133 - acc: 0.656 - ETA: 0s - loss: 0.5521 - acc: 0.710 - ETA: 0s - loss: 0.5692 - acc: 0.699 - ETA: 0s - loss: 0.5521 - acc: 0.719 - ETA: 0s - loss: 0.5394 - acc: 0.733 - ETA: 0s - loss: 0.5510 - acc: 0.726 - ETA: 0s - loss: 0.5510 - acc: 0.723 - ETA: 0s - loss: 0.5466 - acc: 0.728 - ETA: 0s - loss: 0.5407 - acc: 0.735 - ETA: 0s - loss: 0.5427 - acc: 0.734 - ETA: 0s - loss: 0.5447 - acc: 0.731 - 1s 183us/sample - loss: 0.5471 - acc: 0.7307 - val_loss: 0.6037 - val_acc: 0.6601\n",
      "Epoch 22/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.6213 - acc: 0.625 - ETA: 0s - loss: 0.5269 - acc: 0.743 - ETA: 0s - loss: 0.5435 - acc: 0.730 - ETA: 0s - loss: 0.5349 - acc: 0.736 - ETA: 0s - loss: 0.5286 - acc: 0.740 - ETA: 0s - loss: 0.5377 - acc: 0.735 - ETA: 0s - loss: 0.5327 - acc: 0.738 - ETA: 0s - loss: 0.5402 - acc: 0.733 - ETA: 0s - loss: 0.5388 - acc: 0.738 - ETA: 0s - loss: 0.5398 - acc: 0.738 - ETA: 0s - loss: 0.5410 - acc: 0.735 - 1s 172us/sample - loss: 0.5411 - acc: 0.7362 - val_loss: 0.6142 - val_acc: 0.6713\n",
      "Epoch 23/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6189 - acc: 0.625 - ETA: 0s - loss: 0.5757 - acc: 0.710 - ETA: 0s - loss: 0.5625 - acc: 0.724 - ETA: 0s - loss: 0.5622 - acc: 0.717 - ETA: 0s - loss: 0.5525 - acc: 0.727 - ETA: 0s - loss: 0.5453 - acc: 0.730 - ETA: 0s - loss: 0.5317 - acc: 0.740 - ETA: 0s - loss: 0.5324 - acc: 0.737 - ETA: 0s - loss: 0.5378 - acc: 0.734 - ETA: 0s - loss: 0.5391 - acc: 0.734 - ETA: 0s - loss: 0.5423 - acc: 0.733 - ETA: 0s - loss: 0.5402 - acc: 0.731 - ETA: 0s - loss: 0.5370 - acc: 0.733 - 1s 213us/sample - loss: 0.5379 - acc: 0.7319 - val_loss: 0.6229 - val_acc: 0.6609\n",
      "Epoch 24/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5727 - acc: 0.656 - ETA: 0s - loss: 0.5113 - acc: 0.736 - ETA: 0s - loss: 0.5385 - acc: 0.739 - ETA: 0s - loss: 0.5366 - acc: 0.738 - ETA: 0s - loss: 0.5330 - acc: 0.740 - ETA: 0s - loss: 0.5303 - acc: 0.739 - ETA: 0s - loss: 0.5293 - acc: 0.740 - ETA: 0s - loss: 0.5229 - acc: 0.746 - ETA: 0s - loss: 0.5239 - acc: 0.744 - ETA: 0s - loss: 0.5278 - acc: 0.741 - ETA: 0s - loss: 0.5310 - acc: 0.738 - 1s 177us/sample - loss: 0.5303 - acc: 0.7382 - val_loss: 0.6249 - val_acc: 0.6609\n",
      "Epoch 25/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5059 - acc: 0.781 - ETA: 0s - loss: 0.5333 - acc: 0.724 - ETA: 0s - loss: 0.5411 - acc: 0.710 - ETA: 0s - loss: 0.5179 - acc: 0.737 - ETA: 0s - loss: 0.5195 - acc: 0.741 - ETA: 0s - loss: 0.5274 - acc: 0.733 - ETA: 0s - loss: 0.5274 - acc: 0.736 - ETA: 0s - loss: 0.5242 - acc: 0.740 - ETA: 0s - loss: 0.5237 - acc: 0.739 - ETA: 0s - loss: 0.5319 - acc: 0.733 - ETA: 0s - loss: 0.5326 - acc: 0.732 - ETA: 0s - loss: 0.5327 - acc: 0.732 - ETA: 0s - loss: 0.5344 - acc: 0.731 - ETA: 0s - loss: 0.5341 - acc: 0.732 - ETA: 0s - loss: 0.5326 - acc: 0.733 - 1s 245us/sample - loss: 0.5326 - acc: 0.7339 - val_loss: 0.6138 - val_acc: 0.6756\n",
      "Epoch 26/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.4874 - acc: 0.781 - ETA: 0s - loss: 0.5246 - acc: 0.767 - ETA: 0s - loss: 0.5437 - acc: 0.741 - ETA: 0s - loss: 0.5220 - acc: 0.753 - ETA: 0s - loss: 0.5238 - acc: 0.746 - ETA: 0s - loss: 0.5144 - acc: 0.749 - ETA: 0s - loss: 0.5260 - acc: 0.737 - ETA: 0s - loss: 0.5230 - acc: 0.743 - ETA: 0s - loss: 0.5223 - acc: 0.744 - ETA: 0s - loss: 0.5226 - acc: 0.744 - ETA: 0s - loss: 0.5232 - acc: 0.743 - ETA: 0s - loss: 0.5247 - acc: 0.742 - 1s 191us/sample - loss: 0.5248 - acc: 0.7428 - val_loss: 0.6303 - val_acc: 0.6678\n",
      "Epoch 27/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6318 - acc: 0.562 - ETA: 0s - loss: 0.5370 - acc: 0.715 - ETA: 0s - loss: 0.5412 - acc: 0.718 - ETA: 0s - loss: 0.5255 - acc: 0.735 - ETA: 0s - loss: 0.5245 - acc: 0.733 - ETA: 0s - loss: 0.5200 - acc: 0.739 - ETA: 0s - loss: 0.5215 - acc: 0.739 - ETA: 0s - loss: 0.5193 - acc: 0.739 - ETA: 0s - loss: 0.5224 - acc: 0.737 - ETA: 0s - loss: 0.5216 - acc: 0.738 - ETA: 0s - loss: 0.5258 - acc: 0.735 - ETA: 0s - loss: 0.5302 - acc: 0.732 - ETA: 0s - loss: 0.5251 - acc: 0.739 - ETA: 0s - loss: 0.5211 - acc: 0.742 - 1s 247us/sample - loss: 0.5222 - acc: 0.7417 - val_loss: 0.6434 - val_acc: 0.6687\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_1_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_2_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_3_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_4_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_5_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_6_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-input_units: 46</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-no. Of Hidden Layers: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.688524603843689</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3476 samples, validate on 1159 samples\n",
      "Epoch 1/500\n",
      "3476/3476 [==============================] - ETA: 1:57 - loss: 0.6953 - acc: 0.406 - ETA: 19s - loss: 0.6578 - acc: 0.625 - ETA: 8s - loss: 0.6437 - acc: 0.6562 - ETA: 5s - loss: 0.6324 - acc: 0.672 - ETA: 3s - loss: 0.6381 - acc: 0.667 - ETA: 2s - loss: 0.6325 - acc: 0.674 - ETA: 2s - loss: 0.6305 - acc: 0.677 - ETA: 2s - loss: 0.6327 - acc: 0.675 - ETA: 2s - loss: 0.6303 - acc: 0.676 - ETA: 1s - loss: 0.6226 - acc: 0.683 - ETA: 1s - loss: 0.6216 - acc: 0.683 - ETA: 1s - loss: 0.6233 - acc: 0.681 - ETA: 0s - loss: 0.6227 - acc: 0.682 - ETA: 0s - loss: 0.6217 - acc: 0.682 - ETA: 0s - loss: 0.6242 - acc: 0.682 - ETA: 0s - loss: 0.6264 - acc: 0.678 - ETA: 0s - loss: 0.6251 - acc: 0.680 - ETA: 0s - loss: 0.6252 - acc: 0.679 - ETA: 0s - loss: 0.6217 - acc: 0.681 - ETA: 0s - loss: 0.6220 - acc: 0.680 - 3s 756us/sample - loss: 0.6225 - acc: 0.6804 - val_loss: 0.6153 - val_acc: 0.6644\n",
      "Epoch 2/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6304 - acc: 0.656 - ETA: 1s - loss: 0.5884 - acc: 0.725 - ETA: 0s - loss: 0.5840 - acc: 0.718 - ETA: 0s - loss: 0.5897 - acc: 0.706 - ETA: 0s - loss: 0.5913 - acc: 0.700 - ETA: 0s - loss: 0.5951 - acc: 0.694 - ETA: 0s - loss: 0.6031 - acc: 0.687 - ETA: 0s - loss: 0.6062 - acc: 0.684 - ETA: 0s - loss: 0.6011 - acc: 0.685 - ETA: 0s - loss: 0.5991 - acc: 0.688 - ETA: 0s - loss: 0.5982 - acc: 0.687 - ETA: 0s - loss: 0.5956 - acc: 0.691 - ETA: 0s - loss: 0.5995 - acc: 0.688 - ETA: 0s - loss: 0.6001 - acc: 0.687 - ETA: 0s - loss: 0.6028 - acc: 0.684 - 1s 252us/sample - loss: 0.6020 - acc: 0.6867 - val_loss: 0.6124 - val_acc: 0.6618\n",
      "Epoch 3/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.7198 - acc: 0.593 - ETA: 0s - loss: 0.6094 - acc: 0.683 - ETA: 0s - loss: 0.5750 - acc: 0.716 - ETA: 0s - loss: 0.5716 - acc: 0.715 - ETA: 0s - loss: 0.5788 - acc: 0.706 - ETA: 0s - loss: 0.5806 - acc: 0.707 - ETA: 0s - loss: 0.5816 - acc: 0.704 - ETA: 0s - loss: 0.5886 - acc: 0.694 - ETA: 0s - loss: 0.5849 - acc: 0.700 - ETA: 0s - loss: 0.5835 - acc: 0.705 - ETA: 0s - loss: 0.5819 - acc: 0.707 - ETA: 0s - loss: 0.5840 - acc: 0.704 - ETA: 0s - loss: 0.5858 - acc: 0.703 - ETA: 0s - loss: 0.5845 - acc: 0.702 - ETA: 0s - loss: 0.5881 - acc: 0.698 - ETA: 0s - loss: 0.5903 - acc: 0.695 - ETA: 0s - loss: 0.5911 - acc: 0.694 - 1s 293us/sample - loss: 0.5921 - acc: 0.6945 - val_loss: 0.6137 - val_acc: 0.6678\n",
      "Epoch 4/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6316 - acc: 0.687 - ETA: 0s - loss: 0.5712 - acc: 0.708 - ETA: 0s - loss: 0.5762 - acc: 0.710 - ETA: 0s - loss: 0.5654 - acc: 0.718 - ETA: 0s - loss: 0.5777 - acc: 0.710 - ETA: 0s - loss: 0.5678 - acc: 0.714 - ETA: 0s - loss: 0.5723 - acc: 0.711 - ETA: 0s - loss: 0.5758 - acc: 0.705 - ETA: 0s - loss: 0.5784 - acc: 0.704 - ETA: 0s - loss: 0.5821 - acc: 0.700 - ETA: 0s - loss: 0.5769 - acc: 0.706 - ETA: 0s - loss: 0.5819 - acc: 0.702 - ETA: 0s - loss: 0.5837 - acc: 0.702 - ETA: 0s - loss: 0.5868 - acc: 0.699 - ETA: 0s - loss: 0.5870 - acc: 0.698 - ETA: 0s - loss: 0.5892 - acc: 0.695 - 1s 269us/sample - loss: 0.5866 - acc: 0.6962 - val_loss: 0.6049 - val_acc: 0.6678\n",
      "Epoch 5/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5940 - acc: 0.593 - ETA: 0s - loss: 0.5727 - acc: 0.722 - ETA: 0s - loss: 0.5714 - acc: 0.715 - ETA: 0s - loss: 0.5858 - acc: 0.700 - ETA: 0s - loss: 0.5815 - acc: 0.702 - ETA: 0s - loss: 0.5658 - acc: 0.714 - ETA: 0s - loss: 0.5729 - acc: 0.713 - ETA: 0s - loss: 0.5735 - acc: 0.714 - ETA: 0s - loss: 0.5804 - acc: 0.706 - ETA: 0s - loss: 0.5752 - acc: 0.709 - ETA: 0s - loss: 0.5775 - acc: 0.706 - ETA: 0s - loss: 0.5829 - acc: 0.700 - ETA: 0s - loss: 0.5848 - acc: 0.697 - ETA: 0s - loss: 0.5855 - acc: 0.696 - 1s 225us/sample - loss: 0.5858 - acc: 0.6959 - val_loss: 0.6056 - val_acc: 0.6661\n",
      "Epoch 6/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5471 - acc: 0.718 - ETA: 0s - loss: 0.5611 - acc: 0.701 - ETA: 0s - loss: 0.5654 - acc: 0.703 - ETA: 1s - loss: 0.5595 - acc: 0.703 - ETA: 1s - loss: 0.5640 - acc: 0.702 - ETA: 1s - loss: 0.5561 - acc: 0.708 - ETA: 1s - loss: 0.5643 - acc: 0.704 - ETA: 1s - loss: 0.5647 - acc: 0.704 - ETA: 1s - loss: 0.5652 - acc: 0.706 - ETA: 1s - loss: 0.5696 - acc: 0.703 - ETA: 0s - loss: 0.5702 - acc: 0.701 - ETA: 0s - loss: 0.5729 - acc: 0.702 - ETA: 0s - loss: 0.5754 - acc: 0.707 - ETA: 0s - loss: 0.5770 - acc: 0.708 - ETA: 0s - loss: 0.5787 - acc: 0.704 - ETA: 0s - loss: 0.5782 - acc: 0.704 - ETA: 0s - loss: 0.5788 - acc: 0.702 - ETA: 0s - loss: 0.5797 - acc: 0.702 - ETA: 0s - loss: 0.5788 - acc: 0.703 - ETA: 0s - loss: 0.5808 - acc: 0.701 - ETA: 0s - loss: 0.5808 - acc: 0.700 - ETA: 0s - loss: 0.5800 - acc: 0.701 - 1s 370us/sample - loss: 0.5806 - acc: 0.7002 - val_loss: 0.6011 - val_acc: 0.6756\n",
      "Epoch 7/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5835 - acc: 0.656 - ETA: 0s - loss: 0.5517 - acc: 0.703 - ETA: 0s - loss: 0.5692 - acc: 0.691 - ETA: 0s - loss: 0.5695 - acc: 0.688 - ETA: 0s - loss: 0.5671 - acc: 0.699 - ETA: 0s - loss: 0.5632 - acc: 0.708 - ETA: 0s - loss: 0.5602 - acc: 0.715 - ETA: 0s - loss: 0.5637 - acc: 0.715 - ETA: 0s - loss: 0.5645 - acc: 0.713 - ETA: 0s - loss: 0.5637 - acc: 0.714 - ETA: 0s - loss: 0.5663 - acc: 0.714 - ETA: 0s - loss: 0.5680 - acc: 0.714 - ETA: 0s - loss: 0.5663 - acc: 0.714 - ETA: 0s - loss: 0.5670 - acc: 0.714 - ETA: 0s - loss: 0.5679 - acc: 0.710 - ETA: 0s - loss: 0.5696 - acc: 0.706 - ETA: 0s - loss: 0.5698 - acc: 0.704 - ETA: 0s - loss: 0.5716 - acc: 0.702 - 1s 290us/sample - loss: 0.5732 - acc: 0.7005 - val_loss: 0.6061 - val_acc: 0.6670\n",
      "Epoch 8/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5394 - acc: 0.781 - ETA: 0s - loss: 0.5611 - acc: 0.718 - ETA: 0s - loss: 0.5668 - acc: 0.707 - ETA: 0s - loss: 0.5798 - acc: 0.696 - ETA: 0s - loss: 0.5737 - acc: 0.702 - ETA: 0s - loss: 0.5660 - acc: 0.708 - ETA: 0s - loss: 0.5621 - acc: 0.708 - ETA: 0s - loss: 0.5673 - acc: 0.709 - ETA: 0s - loss: 0.5647 - acc: 0.712 - ETA: 0s - loss: 0.5682 - acc: 0.707 - ETA: 0s - loss: 0.5686 - acc: 0.708 - ETA: 0s - loss: 0.5697 - acc: 0.705 - ETA: 0s - loss: 0.5679 - acc: 0.709 - ETA: 0s - loss: 0.5661 - acc: 0.712 - ETA: 0s - loss: 0.5655 - acc: 0.710 - ETA: 0s - loss: 0.5662 - acc: 0.710 - 1s 261us/sample - loss: 0.5688 - acc: 0.7097 - val_loss: 0.6015 - val_acc: 0.6756\n",
      "Epoch 9/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5830 - acc: 0.781 - ETA: 0s - loss: 0.5691 - acc: 0.723 - ETA: 0s - loss: 0.5931 - acc: 0.697 - ETA: 0s - loss: 0.5813 - acc: 0.704 - ETA: 0s - loss: 0.5794 - acc: 0.703 - ETA: 0s - loss: 0.5783 - acc: 0.700 - ETA: 0s - loss: 0.5725 - acc: 0.701 - ETA: 0s - loss: 0.5652 - acc: 0.708 - ETA: 0s - loss: 0.5627 - acc: 0.708 - ETA: 0s - loss: 0.5641 - acc: 0.709 - ETA: 0s - loss: 0.5682 - acc: 0.708 - ETA: 0s - loss: 0.5660 - acc: 0.713 - ETA: 0s - loss: 0.5681 - acc: 0.711 - ETA: 0s - loss: 0.5697 - acc: 0.713 - ETA: 0s - loss: 0.5716 - acc: 0.710 - ETA: 0s - loss: 0.5723 - acc: 0.708 - 1s 277us/sample - loss: 0.5700 - acc: 0.7089 - val_loss: 0.6091 - val_acc: 0.6790\n",
      "Epoch 10/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5858 - acc: 0.656 - ETA: 0s - loss: 0.6051 - acc: 0.677 - ETA: 0s - loss: 0.5988 - acc: 0.674 - ETA: 0s - loss: 0.5935 - acc: 0.679 - ETA: 0s - loss: 0.5814 - acc: 0.699 - ETA: 0s - loss: 0.5768 - acc: 0.704 - ETA: 0s - loss: 0.5639 - acc: 0.716 - ETA: 0s - loss: 0.5573 - acc: 0.720 - ETA: 0s - loss: 0.5528 - acc: 0.722 - ETA: 0s - loss: 0.5555 - acc: 0.721 - ETA: 0s - loss: 0.5584 - acc: 0.716 - ETA: 0s - loss: 0.5601 - acc: 0.715 - ETA: 0s - loss: 0.5585 - acc: 0.717 - ETA: 0s - loss: 0.5607 - acc: 0.713 - 1s 240us/sample - loss: 0.5589 - acc: 0.7152 - val_loss: 0.6035 - val_acc: 0.6713\n",
      "Epoch 11/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.5857 - acc: 0.625 - ETA: 1s - loss: 0.5589 - acc: 0.696 - ETA: 0s - loss: 0.5405 - acc: 0.718 - ETA: 0s - loss: 0.5496 - acc: 0.713 - ETA: 0s - loss: 0.5545 - acc: 0.710 - ETA: 0s - loss: 0.5571 - acc: 0.703 - ETA: 0s - loss: 0.5542 - acc: 0.706 - ETA: 0s - loss: 0.5503 - acc: 0.712 - ETA: 0s - loss: 0.5446 - acc: 0.720 - ETA: 0s - loss: 0.5451 - acc: 0.718 - ETA: 0s - loss: 0.5462 - acc: 0.718 - ETA: 0s - loss: 0.5473 - acc: 0.719 - ETA: 0s - loss: 0.5475 - acc: 0.719 - ETA: 0s - loss: 0.5514 - acc: 0.717 - ETA: 0s - loss: 0.5534 - acc: 0.714 - ETA: 0s - loss: 0.5547 - acc: 0.714 - ETA: 0s - loss: 0.5538 - acc: 0.716 - 1s 277us/sample - loss: 0.5533 - acc: 0.7169 - val_loss: 0.6379 - val_acc: 0.6609\n",
      "Epoch 12/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.6749 - acc: 0.718 - ETA: 0s - loss: 0.5833 - acc: 0.704 - ETA: 0s - loss: 0.5488 - acc: 0.718 - ETA: 0s - loss: 0.5419 - acc: 0.723 - ETA: 0s - loss: 0.5494 - acc: 0.718 - ETA: 0s - loss: 0.5420 - acc: 0.720 - ETA: 0s - loss: 0.5452 - acc: 0.715 - ETA: 0s - loss: 0.5440 - acc: 0.718 - ETA: 0s - loss: 0.5411 - acc: 0.722 - ETA: 0s - loss: 0.5400 - acc: 0.726 - ETA: 0s - loss: 0.5429 - acc: 0.722 - ETA: 0s - loss: 0.5449 - acc: 0.721 - ETA: 0s - loss: 0.5467 - acc: 0.720 - ETA: 0s - loss: 0.5442 - acc: 0.722 - 1s 229us/sample - loss: 0.5483 - acc: 0.7201 - val_loss: 0.6068 - val_acc: 0.6575\n",
      "Epoch 13/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.5045 - acc: 0.781 - ETA: 0s - loss: 0.5109 - acc: 0.750 - ETA: 0s - loss: 0.5542 - acc: 0.718 - ETA: 0s - loss: 0.5575 - acc: 0.709 - ETA: 0s - loss: 0.5446 - acc: 0.727 - ETA: 0s - loss: 0.5504 - acc: 0.722 - ETA: 0s - loss: 0.5436 - acc: 0.723 - ETA: 0s - loss: 0.5457 - acc: 0.721 - ETA: 0s - loss: 0.5531 - acc: 0.715 - ETA: 0s - loss: 0.5538 - acc: 0.714 - ETA: 0s - loss: 0.5577 - acc: 0.712 - ETA: 0s - loss: 0.5540 - acc: 0.711 - ETA: 0s - loss: 0.5539 - acc: 0.710 - ETA: 0s - loss: 0.5540 - acc: 0.711 - ETA: 0s - loss: 0.5517 - acc: 0.713 - ETA: 0s - loss: 0.5467 - acc: 0.719 - ETA: 0s - loss: 0.5423 - acc: 0.722 - ETA: 0s - loss: 0.5411 - acc: 0.725 - ETA: 0s - loss: 0.5409 - acc: 0.725 - 1s 309us/sample - loss: 0.5437 - acc: 0.7235 - val_loss: 0.6170 - val_acc: 0.6704\n",
      "Epoch 14/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.4204 - acc: 0.812 - ETA: 1s - loss: 0.5422 - acc: 0.706 - ETA: 0s - loss: 0.5377 - acc: 0.734 - ETA: 0s - loss: 0.5430 - acc: 0.716 - ETA: 0s - loss: 0.5364 - acc: 0.725 - ETA: 0s - loss: 0.5317 - acc: 0.732 - ETA: 0s - loss: 0.5321 - acc: 0.729 - ETA: 0s - loss: 0.5299 - acc: 0.730 - ETA: 0s - loss: 0.5317 - acc: 0.727 - ETA: 0s - loss: 0.5309 - acc: 0.730 - ETA: 0s - loss: 0.5282 - acc: 0.734 - ETA: 0s - loss: 0.5342 - acc: 0.731 - ETA: 0s - loss: 0.5310 - acc: 0.735 - ETA: 0s - loss: 0.5329 - acc: 0.733 - ETA: 0s - loss: 0.5342 - acc: 0.732 - ETA: 0s - loss: 0.5336 - acc: 0.733 - ETA: 0s - loss: 0.5308 - acc: 0.734 - ETA: 0s - loss: 0.5317 - acc: 0.731 - ETA: 0s - loss: 0.5348 - acc: 0.729 - ETA: 0s - loss: 0.5317 - acc: 0.732 - ETA: 0s - loss: 0.5342 - acc: 0.730 - ETA: 0s - loss: 0.5351 - acc: 0.728 - 1s 363us/sample - loss: 0.5366 - acc: 0.7270 - val_loss: 0.6191 - val_acc: 0.6773\n",
      "Epoch 15/500\n",
      "3476/3476 [==============================] - ETA: 0s - loss: 0.5532 - acc: 0.750 - ETA: 1s - loss: 0.5081 - acc: 0.750 - ETA: 1s - loss: 0.4839 - acc: 0.756 - ETA: 1s - loss: 0.4991 - acc: 0.761 - ETA: 1s - loss: 0.5184 - acc: 0.736 - ETA: 0s - loss: 0.5106 - acc: 0.748 - ETA: 0s - loss: 0.5079 - acc: 0.753 - ETA: 0s - loss: 0.5085 - acc: 0.752 - ETA: 0s - loss: 0.5050 - acc: 0.753 - ETA: 0s - loss: 0.5096 - acc: 0.749 - ETA: 0s - loss: 0.5100 - acc: 0.748 - ETA: 0s - loss: 0.5174 - acc: 0.743 - ETA: 0s - loss: 0.5203 - acc: 0.741 - ETA: 0s - loss: 0.5186 - acc: 0.744 - ETA: 0s - loss: 0.5215 - acc: 0.739 - ETA: 0s - loss: 0.5268 - acc: 0.735 - ETA: 0s - loss: 0.5295 - acc: 0.734 - ETA: 0s - loss: 0.5311 - acc: 0.733 - ETA: 0s - loss: 0.5293 - acc: 0.733 - ETA: 0s - loss: 0.5330 - acc: 0.730 - ETA: 0s - loss: 0.5332 - acc: 0.731 - ETA: 0s - loss: 0.5321 - acc: 0.731 - ETA: 0s - loss: 0.5335 - acc: 0.729 - ETA: 0s - loss: 0.5299 - acc: 0.732 - 1s 383us/sample - loss: 0.5291 - acc: 0.7325 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 16/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.6314 - acc: 0.593 - ETA: 1s - loss: 0.5492 - acc: 0.700 - ETA: 1s - loss: 0.5348 - acc: 0.698 - ETA: 1s - loss: 0.5424 - acc: 0.697 - ETA: 0s - loss: 0.5431 - acc: 0.708 - ETA: 0s - loss: 0.5437 - acc: 0.711 - ETA: 0s - loss: 0.5434 - acc: 0.719 - ETA: 0s - loss: 0.5371 - acc: 0.725 - ETA: 0s - loss: 0.5373 - acc: 0.722 - ETA: 0s - loss: 0.5322 - acc: 0.725 - ETA: 0s - loss: 0.5245 - acc: 0.736 - ETA: 0s - loss: 0.5227 - acc: 0.737 - ETA: 0s - loss: 0.5200 - acc: 0.738 - ETA: 0s - loss: 0.5175 - acc: 0.742 - ETA: 0s - loss: 0.5230 - acc: 0.740 - ETA: 0s - loss: 0.5216 - acc: 0.741 - ETA: 0s - loss: 0.5242 - acc: 0.738 - ETA: 0s - loss: 0.5233 - acc: 0.741 - ETA: 0s - loss: 0.5235 - acc: 0.742 - ETA: 0s - loss: 0.5225 - acc: 0.742 - ETA: 0s - loss: 0.5234 - acc: 0.739 - ETA: 0s - loss: 0.5224 - acc: 0.738 - 1s 365us/sample - loss: 0.5230 - acc: 0.7388 - val_loss: 0.6400 - val_acc: 0.6557\n",
      "Epoch 17/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.3836 - acc: 0.781 - ETA: 1s - loss: 0.4612 - acc: 0.762 - ETA: 1s - loss: 0.4750 - acc: 0.756 - ETA: 1s - loss: 0.4826 - acc: 0.752 - ETA: 1s - loss: 0.4851 - acc: 0.751 - ETA: 1s - loss: 0.4914 - acc: 0.748 - ETA: 1s - loss: 0.4939 - acc: 0.748 - ETA: 1s - loss: 0.4954 - acc: 0.748 - ETA: 1s - loss: 0.4966 - acc: 0.745 - ETA: 1s - loss: 0.4958 - acc: 0.745 - ETA: 1s - loss: 0.4943 - acc: 0.746 - ETA: 0s - loss: 0.4836 - acc: 0.753 - ETA: 0s - loss: 0.4926 - acc: 0.746 - ETA: 0s - loss: 0.4959 - acc: 0.745 - ETA: 0s - loss: 0.4998 - acc: 0.741 - ETA: 0s - loss: 0.5079 - acc: 0.732 - ETA: 0s - loss: 0.5092 - acc: 0.727 - ETA: 0s - loss: 0.5089 - acc: 0.731 - ETA: 0s - loss: 0.5119 - acc: 0.731 - ETA: 0s - loss: 0.5140 - acc: 0.732 - ETA: 0s - loss: 0.5145 - acc: 0.734 - ETA: 0s - loss: 0.5123 - acc: 0.735 - ETA: 0s - loss: 0.5143 - acc: 0.734 - 1s 391us/sample - loss: 0.5165 - acc: 0.7327 - val_loss: 0.6343 - val_acc: 0.6678\n",
      "Epoch 18/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.5156 - acc: 0.781 - ETA: 1s - loss: 0.5237 - acc: 0.724 - ETA: 1s - loss: 0.5185 - acc: 0.731 - ETA: 0s - loss: 0.5233 - acc: 0.733 - ETA: 0s - loss: 0.5071 - acc: 0.742 - ETA: 0s - loss: 0.4996 - acc: 0.744 - ETA: 0s - loss: 0.5039 - acc: 0.737 - ETA: 0s - loss: 0.5020 - acc: 0.740 - ETA: 0s - loss: 0.5002 - acc: 0.742 - ETA: 0s - loss: 0.5062 - acc: 0.738 - ETA: 0s - loss: 0.5033 - acc: 0.741 - ETA: 0s - loss: 0.5041 - acc: 0.744 - ETA: 0s - loss: 0.5015 - acc: 0.747 - ETA: 0s - loss: 0.5016 - acc: 0.748 - ETA: 0s - loss: 0.4981 - acc: 0.749 - ETA: 0s - loss: 0.5023 - acc: 0.745 - ETA: 0s - loss: 0.5007 - acc: 0.747 - ETA: 0s - loss: 0.5021 - acc: 0.746 - ETA: 0s - loss: 0.5023 - acc: 0.748 - ETA: 0s - loss: 0.5017 - acc: 0.749 - ETA: 0s - loss: 0.5033 - acc: 0.748 - ETA: 0s - loss: 0.5018 - acc: 0.748 - ETA: 0s - loss: 0.5007 - acc: 0.748 - 1s 385us/sample - loss: 0.5019 - acc: 0.7480 - val_loss: 0.6415 - val_acc: 0.6652\n",
      "Epoch 19/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.5894 - acc: 0.656 - ETA: 1s - loss: 0.4797 - acc: 0.741 - ETA: 1s - loss: 0.4922 - acc: 0.742 - ETA: 0s - loss: 0.4950 - acc: 0.737 - ETA: 0s - loss: 0.4859 - acc: 0.751 - ETA: 0s - loss: 0.4868 - acc: 0.752 - ETA: 0s - loss: 0.4815 - acc: 0.750 - ETA: 0s - loss: 0.4676 - acc: 0.761 - ETA: 0s - loss: 0.4700 - acc: 0.762 - ETA: 0s - loss: 0.4811 - acc: 0.755 - ETA: 0s - loss: 0.4807 - acc: 0.756 - ETA: 0s - loss: 0.4873 - acc: 0.749 - ETA: 0s - loss: 0.4866 - acc: 0.750 - ETA: 0s - loss: 0.4884 - acc: 0.748 - ETA: 0s - loss: 0.4912 - acc: 0.746 - ETA: 0s - loss: 0.4912 - acc: 0.747 - ETA: 0s - loss: 0.4923 - acc: 0.746 - ETA: 0s - loss: 0.4900 - acc: 0.748 - ETA: 0s - loss: 0.4925 - acc: 0.747 - 1s 303us/sample - loss: 0.4939 - acc: 0.7465 - val_loss: 0.6625 - val_acc: 0.6480\n",
      "Epoch 20/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.4952 - acc: 0.687 - ETA: 1s - loss: 0.5014 - acc: 0.710 - ETA: 1s - loss: 0.5163 - acc: 0.725 - ETA: 1s - loss: 0.5219 - acc: 0.731 - ETA: 1s - loss: 0.5218 - acc: 0.722 - ETA: 1s - loss: 0.5150 - acc: 0.731 - ETA: 0s - loss: 0.5105 - acc: 0.736 - ETA: 0s - loss: 0.5053 - acc: 0.741 - ETA: 0s - loss: 0.4972 - acc: 0.748 - ETA: 0s - loss: 0.5026 - acc: 0.749 - ETA: 0s - loss: 0.4988 - acc: 0.755 - ETA: 0s - loss: 0.4927 - acc: 0.757 - ETA: 0s - loss: 0.4923 - acc: 0.756 - ETA: 0s - loss: 0.4919 - acc: 0.758 - ETA: 0s - loss: 0.4906 - acc: 0.755 - ETA: 0s - loss: 0.4900 - acc: 0.753 - ETA: 0s - loss: 0.4904 - acc: 0.755 - ETA: 0s - loss: 0.4872 - acc: 0.756 - ETA: 0s - loss: 0.4862 - acc: 0.756 - ETA: 0s - loss: 0.4866 - acc: 0.758 - 1s 330us/sample - loss: 0.4893 - acc: 0.7558 - val_loss: 0.6510 - val_acc: 0.6506\n",
      "Epoch 21/500\n",
      "3476/3476 [==============================] - ETA: 2s - loss: 0.4138 - acc: 0.812 - ETA: 0s - loss: 0.4754 - acc: 0.798 - ETA: 0s - loss: 0.4473 - acc: 0.798 - ETA: 0s - loss: 0.4520 - acc: 0.791 - ETA: 0s - loss: 0.4561 - acc: 0.781 - ETA: 0s - loss: 0.4628 - acc: 0.775 - ETA: 0s - loss: 0.4706 - acc: 0.768 - ETA: 0s - loss: 0.4725 - acc: 0.766 - ETA: 0s - loss: 0.4694 - acc: 0.770 - ETA: 0s - loss: 0.4693 - acc: 0.770 - ETA: 0s - loss: 0.4746 - acc: 0.766 - ETA: 0s - loss: 0.4705 - acc: 0.767 - ETA: 0s - loss: 0.4711 - acc: 0.767 - ETA: 0s - loss: 0.4682 - acc: 0.770 - ETA: 0s - loss: 0.4734 - acc: 0.765 - ETA: 0s - loss: 0.4733 - acc: 0.766 - 1s 264us/sample - loss: 0.4721 - acc: 0.7655 - val_loss: 0.7043 - val_acc: 0.6575\n",
      "Epoch 22/500\n",
      "3476/3476 [==============================] - ETA: 1s - loss: 0.4677 - acc: 0.750 - ETA: 0s - loss: 0.4651 - acc: 0.773 - ETA: 0s - loss: 0.4339 - acc: 0.794 - ETA: 0s - loss: 0.4502 - acc: 0.786 - ETA: 0s - loss: 0.4591 - acc: 0.779 - ETA: 0s - loss: 0.4622 - acc: 0.779 - ETA: 0s - loss: 0.4672 - acc: 0.771 - ETA: 0s - loss: 0.4603 - acc: 0.778 - ETA: 0s - loss: 0.4507 - acc: 0.780 - ETA: 0s - loss: 0.4486 - acc: 0.782 - ETA: 0s - loss: 0.4566 - acc: 0.777 - ETA: 0s - loss: 0.4583 - acc: 0.777 - ETA: 0s - loss: 0.4590 - acc: 0.777 - ETA: 0s - loss: 0.4621 - acc: 0.776 - ETA: 0s - loss: 0.4627 - acc: 0.776 - ETA: 0s - loss: 0.4642 - acc: 0.774 - ETA: 0s - loss: 0.4658 - acc: 0.774 - ETA: 0s - loss: 0.4680 - acc: 0.773 - ETA: 0s - loss: 0.4684 - acc: 0.771 - 1s 305us/sample - loss: 0.4660 - acc: 0.7733 - val_loss: 0.7524 - val_acc: 0.6471\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_1_units: 222</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_2_units: 206</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_3_units: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_4_units: 94</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_5_units: 206</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_6_units: 78</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-input_units: 46</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-no. Of Hidden Layers: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.679033637046814</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             epochs = 100,\n",
    "#             batch_size = 64,\n",
    "             callbacks=[early_stop],\n",
    "             validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in 1577048990/untitled_project</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_acc', direction='max') Score: 0.688524603843689</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_acc', direction='max') Score: 0.679033637046814</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'input_units': 46, 'no. Of Hidden Layers': 6, 'Hidden_layer_1_units': 30, 'Hidden_layer_2_units': 30, 'Hidden_layer_3_units': 30, 'Hidden_layer_4_units': 30, 'Hidden_layer_5_units': 30, 'Hidden_layer_6_units': 30}\n"
     ]
    }
   ],
   "source": [
    "print(tuner.results_summary()) # top 10 trials\n",
    "print(tuner.get_best_hyperparameters()[0].values) #values of best hyper-parameters\n",
    "#print(tuner.get_best_models()[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
