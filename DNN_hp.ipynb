{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN with Hyper-Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3336, 42) | X_test shape: (835, 42) | y_train shape: (3336,) | y_test shape: (835,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # disable GPU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "\n",
    "#hyper-parameter tuning imports\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\" # folder names as a timestamp\n",
    "\n",
    "SEED = 111 # constant seed for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "df = pd.read_csv(\"UFC_TRAIN.csv\")\n",
    "\n",
    "# train/test split\n",
    "X = df.drop([\"Winner\",\"B_fighter\",\"R_fighter\"], axis=1).values\n",
    "y = df[\"Winner\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape} | X_test shape: {X_test.shape} | y_train shape: {y_train.shape} | y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Parameters\n",
    "MIN = 30\n",
    "MAX = 256\n",
    "STEP = 16\n",
    "MAX_TRIALS = 2\n",
    "EXE_PER_TRIAL = 1\n",
    "EPOCHS = 10\n",
    "PATIENCE = 16\n",
    "\n",
    "# function to build the model (argument: hyper-parameter)\n",
    "def build_model(hp):\n",
    "    # model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first layer's no. of neurons = hp.Int range of values to test\n",
    "    model.add(Dense(hp.Int(\"input_units\", min_value=MIN, max_value=MAX, step=STEP), activation='relu'))\n",
    "    \n",
    "    # range of 1 to 10 layers to test\n",
    "    for i in range(hp.Int(\"no. Of Hidden Layers\", 1, 10)):\n",
    "        # for each added layer, again test range of neurons and add dropout\n",
    "        model.add(Dense(hp.Int(f\"Hidden_layer_{i+1}_units\", min_value=MIN, max_value=MAX, step=STEP), activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "        build_model, # name of the function that builds the model\n",
    "        objective=\"val_accuracy\", # the thing that we're interested to trace\n",
    "        max_trials = MAX_TRIALS, # no. of combinations to try\n",
    "        executions_per_trial = EXE_PER_TRIAL, # no. of times to train each combination (true avg)\n",
    "        directory=LOG_DIR) # directory to save the outputs\n",
    "\n",
    "# prevent divergence of loss & val_loss via early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3336 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "3336/3336 [==============================] - ETA: 2:44 - loss: 0.7221 - accuracy: 0.46 - ETA: 22s - loss: 0.7012 - accuracy: 0.5179 - ETA: 13s - loss: 0.6818 - accuracy: 0.570 - ETA: 8s - loss: 0.6727 - accuracy: 0.586 - ETA: 6s - loss: 0.6794 - accuracy: 0.57 - ETA: 5s - loss: 0.6774 - accuracy: 0.58 - ETA: 3s - loss: 0.6750 - accuracy: 0.59 - ETA: 3s - loss: 0.6694 - accuracy: 0.61 - ETA: 2s - loss: 0.6697 - accuracy: 0.61 - ETA: 2s - loss: 0.6703 - accuracy: 0.62 - ETA: 1s - loss: 0.6654 - accuracy: 0.62 - ETA: 1s - loss: 0.6635 - accuracy: 0.63 - ETA: 1s - loss: 0.6630 - accuracy: 0.63 - ETA: 0s - loss: 0.6638 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6600 - accuracy: 0.63 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - 3s 963us/sample - loss: 0.6585 - accuracy: 0.6400 - val_loss: 0.6235 - val_accuracy: 0.7090\n",
      "Epoch 2/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6266 - accuracy: 0.65 - ETA: 0s - loss: 0.6373 - accuracy: 0.66 - ETA: 0s - loss: 0.6320 - accuracy: 0.67 - ETA: 0s - loss: 0.6366 - accuracy: 0.67 - ETA: 0s - loss: 0.6281 - accuracy: 0.68 - ETA: 0s - loss: 0.6350 - accuracy: 0.67 - ETA: 0s - loss: 0.6357 - accuracy: 0.67 - ETA: 0s - loss: 0.6355 - accuracy: 0.67 - ETA: 0s - loss: 0.6322 - accuracy: 0.67 - ETA: 0s - loss: 0.6333 - accuracy: 0.67 - ETA: 0s - loss: 0.6340 - accuracy: 0.67 - ETA: 0s - loss: 0.6355 - accuracy: 0.67 - ETA: 0s - loss: 0.6395 - accuracy: 0.66 - ETA: 0s - loss: 0.6413 - accuracy: 0.66 - ETA: 0s - loss: 0.6412 - accuracy: 0.66 - ETA: 0s - loss: 0.6401 - accuracy: 0.66 - ETA: 0s - loss: 0.6387 - accuracy: 0.66 - ETA: 0s - loss: 0.6363 - accuracy: 0.67 - 1s 327us/sample - loss: 0.6371 - accuracy: 0.6703 - val_loss: 0.6039 - val_accuracy: 0.7090\n",
      "Epoch 3/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6087 - accuracy: 0.68 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.65 - ETA: 0s - loss: 0.6442 - accuracy: 0.65 - ETA: 0s - loss: 0.6447 - accuracy: 0.65 - ETA: 0s - loss: 0.6432 - accuracy: 0.66 - ETA: 0s - loss: 0.6373 - accuracy: 0.67 - ETA: 0s - loss: 0.6351 - accuracy: 0.67 - ETA: 0s - loss: 0.6385 - accuracy: 0.66 - ETA: 0s - loss: 0.6382 - accuracy: 0.67 - ETA: 0s - loss: 0.6384 - accuracy: 0.66 - ETA: 0s - loss: 0.6374 - accuracy: 0.66 - ETA: 0s - loss: 0.6336 - accuracy: 0.67 - ETA: 0s - loss: 0.6324 - accuracy: 0.67 - ETA: 0s - loss: 0.6333 - accuracy: 0.67 - 1s 334us/sample - loss: 0.6352 - accuracy: 0.6718 - val_loss: 0.6032 - val_accuracy: 0.7090\n",
      "Epoch 4/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6032 - accuracy: 0.71 - ETA: 1s - loss: 0.6260 - accuracy: 0.70 - ETA: 1s - loss: 0.6300 - accuracy: 0.67 - ETA: 0s - loss: 0.6243 - accuracy: 0.68 - ETA: 0s - loss: 0.6324 - accuracy: 0.66 - ETA: 0s - loss: 0.6317 - accuracy: 0.66 - ETA: 0s - loss: 0.6305 - accuracy: 0.66 - ETA: 0s - loss: 0.6288 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - ETA: 0s - loss: 0.6267 - accuracy: 0.67 - ETA: 0s - loss: 0.6308 - accuracy: 0.66 - ETA: 0s - loss: 0.6305 - accuracy: 0.66 - ETA: 0s - loss: 0.6303 - accuracy: 0.66 - ETA: 0s - loss: 0.6288 - accuracy: 0.67 - ETA: 0s - loss: 0.6260 - accuracy: 0.67 - ETA: 0s - loss: 0.6246 - accuracy: 0.67 - ETA: 0s - loss: 0.6265 - accuracy: 0.67 - ETA: 0s - loss: 0.6264 - accuracy: 0.67 - ETA: 0s - loss: 0.6266 - accuracy: 0.67 - 1s 357us/sample - loss: 0.6289 - accuracy: 0.6718 - val_loss: 0.6114 - val_accuracy: 0.7090\n",
      "Epoch 5/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6405 - accuracy: 0.65 - ETA: 2s - loss: 0.6585 - accuracy: 0.63 - ETA: 2s - loss: 0.6547 - accuracy: 0.62 - ETA: 2s - loss: 0.6502 - accuracy: 0.62 - ETA: 2s - loss: 0.6488 - accuracy: 0.63 - ETA: 2s - loss: 0.6511 - accuracy: 0.62 - ETA: 2s - loss: 0.6487 - accuracy: 0.62 - ETA: 2s - loss: 0.6450 - accuracy: 0.63 - ETA: 2s - loss: 0.6429 - accuracy: 0.63 - ETA: 2s - loss: 0.6415 - accuracy: 0.63 - ETA: 1s - loss: 0.6470 - accuracy: 0.63 - ETA: 1s - loss: 0.6444 - accuracy: 0.63 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 1s - loss: 0.6400 - accuracy: 0.64 - ETA: 1s - loss: 0.6403 - accuracy: 0.64 - ETA: 1s - loss: 0.6400 - accuracy: 0.64 - ETA: 1s - loss: 0.6385 - accuracy: 0.64 - ETA: 1s - loss: 0.6378 - accuracy: 0.64 - ETA: 1s - loss: 0.6358 - accuracy: 0.65 - ETA: 1s - loss: 0.6331 - accuracy: 0.65 - ETA: 1s - loss: 0.6317 - accuracy: 0.65 - ETA: 1s - loss: 0.6300 - accuracy: 0.66 - ETA: 1s - loss: 0.6316 - accuracy: 0.65 - ETA: 1s - loss: 0.6276 - accuracy: 0.66 - ETA: 1s - loss: 0.6273 - accuracy: 0.66 - ETA: 1s - loss: 0.6298 - accuracy: 0.66 - ETA: 1s - loss: 0.6300 - accuracy: 0.66 - ETA: 1s - loss: 0.6291 - accuracy: 0.66 - ETA: 0s - loss: 0.6278 - accuracy: 0.66 - ETA: 0s - loss: 0.6254 - accuracy: 0.67 - ETA: 0s - loss: 0.6230 - accuracy: 0.67 - ETA: 0s - loss: 0.6217 - accuracy: 0.67 - ETA: 0s - loss: 0.6239 - accuracy: 0.67 - ETA: 0s - loss: 0.6222 - accuracy: 0.67 - ETA: 0s - loss: 0.6236 - accuracy: 0.67 - ETA: 0s - loss: 0.6226 - accuracy: 0.67 - ETA: 0s - loss: 0.6198 - accuracy: 0.67 - ETA: 0s - loss: 0.6193 - accuracy: 0.67 - ETA: 0s - loss: 0.6196 - accuracy: 0.67 - ETA: 0s - loss: 0.6210 - accuracy: 0.67 - 3s 797us/sample - loss: 0.6210 - accuracy: 0.6724 - val_loss: 0.5876 - val_accuracy: 0.7090\n",
      "Epoch 6/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5727 - accuracy: 0.65 - ETA: 0s - loss: 0.6008 - accuracy: 0.67 - ETA: 0s - loss: 0.6007 - accuracy: 0.67 - ETA: 0s - loss: 0.5928 - accuracy: 0.68 - ETA: 0s - loss: 0.6062 - accuracy: 0.67 - ETA: 0s - loss: 0.6069 - accuracy: 0.67 - ETA: 0s - loss: 0.6145 - accuracy: 0.67 - ETA: 0s - loss: 0.6174 - accuracy: 0.66 - ETA: 0s - loss: 0.6186 - accuracy: 0.67 - ETA: 0s - loss: 0.6197 - accuracy: 0.66 - ETA: 0s - loss: 0.6199 - accuracy: 0.66 - ETA: 0s - loss: 0.6179 - accuracy: 0.67 - ETA: 0s - loss: 0.6190 - accuracy: 0.67 - ETA: 0s - loss: 0.6174 - accuracy: 0.67 - ETA: 0s - loss: 0.6179 - accuracy: 0.67 - ETA: 0s - loss: 0.6182 - accuracy: 0.67 - ETA: 0s - loss: 0.6173 - accuracy: 0.67 - ETA: 0s - loss: 0.6160 - accuracy: 0.67 - ETA: 0s - loss: 0.6175 - accuracy: 0.67 - ETA: 0s - loss: 0.6187 - accuracy: 0.66 - ETA: 0s - loss: 0.6195 - accuracy: 0.66 - ETA: 0s - loss: 0.6183 - accuracy: 0.67 - 1s 416us/sample - loss: 0.6179 - accuracy: 0.6724 - val_loss: 0.6018 - val_accuracy: 0.7090\n",
      "Epoch 7/10\n",
      "3336/3336 [==============================] - ETA: 2s - loss: 0.7013 - accuracy: 0.53 - ETA: 1s - loss: 0.6017 - accuracy: 0.68 - ETA: 1s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6257 - accuracy: 0.67 - ETA: 0s - loss: 0.6180 - accuracy: 0.67 - ETA: 0s - loss: 0.6177 - accuracy: 0.67 - ETA: 0s - loss: 0.6132 - accuracy: 0.67 - ETA: 0s - loss: 0.6070 - accuracy: 0.68 - ETA: 0s - loss: 0.6093 - accuracy: 0.68 - ETA: 0s - loss: 0.6098 - accuracy: 0.68 - ETA: 0s - loss: 0.6070 - accuracy: 0.68 - ETA: 0s - loss: 0.6043 - accuracy: 0.68 - ETA: 0s - loss: 0.6105 - accuracy: 0.68 - ETA: 0s - loss: 0.6111 - accuracy: 0.68 - ETA: 0s - loss: 0.6113 - accuracy: 0.68 - ETA: 0s - loss: 0.6119 - accuracy: 0.68 - ETA: 0s - loss: 0.6101 - accuracy: 0.68 - ETA: 0s - loss: 0.6104 - accuracy: 0.68 - ETA: 0s - loss: 0.6087 - accuracy: 0.68 - ETA: 0s - loss: 0.6087 - accuracy: 0.68 - ETA: 0s - loss: 0.6092 - accuracy: 0.68 - ETA: 0s - loss: 0.6088 - accuracy: 0.68 - ETA: 0s - loss: 0.6103 - accuracy: 0.68 - ETA: 0s - loss: 0.6136 - accuracy: 0.67 - ETA: 0s - loss: 0.6149 - accuracy: 0.67 - ETA: 0s - loss: 0.6160 - accuracy: 0.67 - ETA: 0s - loss: 0.6150 - accuracy: 0.67 - ETA: 0s - loss: 0.6158 - accuracy: 0.67 - ETA: 0s - loss: 0.6168 - accuracy: 0.67 - 2s 567us/sample - loss: 0.6178 - accuracy: 0.6724 - val_loss: 0.6029 - val_accuracy: 0.7090\n",
      "Epoch 8/10\n",
      "3336/3336 [==============================] - ETA: 2s - loss: 0.6311 - accuracy: 0.65 - ETA: 1s - loss: 0.6311 - accuracy: 0.63 - ETA: 1s - loss: 0.6097 - accuracy: 0.64 - ETA: 0s - loss: 0.6218 - accuracy: 0.64 - ETA: 0s - loss: 0.6229 - accuracy: 0.65 - ETA: 0s - loss: 0.6200 - accuracy: 0.66 - ETA: 0s - loss: 0.6153 - accuracy: 0.66 - ETA: 0s - loss: 0.6201 - accuracy: 0.66 - ETA: 0s - loss: 0.6206 - accuracy: 0.66 - ETA: 0s - loss: 0.6181 - accuracy: 0.66 - ETA: 0s - loss: 0.6165 - accuracy: 0.66 - ETA: 0s - loss: 0.6179 - accuracy: 0.66 - ETA: 0s - loss: 0.6200 - accuracy: 0.66 - ETA: 0s - loss: 0.6169 - accuracy: 0.66 - ETA: 0s - loss: 0.6152 - accuracy: 0.67 - ETA: 0s - loss: 0.6157 - accuracy: 0.67 - ETA: 0s - loss: 0.6137 - accuracy: 0.67 - ETA: 0s - loss: 0.6146 - accuracy: 0.67 - ETA: 0s - loss: 0.6120 - accuracy: 0.67 - ETA: 0s - loss: 0.6149 - accuracy: 0.67 - ETA: 0s - loss: 0.6136 - accuracy: 0.67 - ETA: 0s - loss: 0.6122 - accuracy: 0.67 - 1s 436us/sample - loss: 0.6131 - accuracy: 0.6724 - val_loss: 0.5978 - val_accuracy: 0.7090\n",
      "Epoch 9/10\n",
      "3336/3336 [==============================] - ETA: 3s - loss: 0.6290 - accuracy: 0.59 - ETA: 2s - loss: 0.6160 - accuracy: 0.65 - ETA: 2s - loss: 0.6091 - accuracy: 0.67 - ETA: 1s - loss: 0.6126 - accuracy: 0.65 - ETA: 1s - loss: 0.6011 - accuracy: 0.67 - ETA: 1s - loss: 0.5979 - accuracy: 0.67 - ETA: 1s - loss: 0.6031 - accuracy: 0.66 - ETA: 1s - loss: 0.6016 - accuracy: 0.66 - ETA: 0s - loss: 0.6014 - accuracy: 0.66 - ETA: 0s - loss: 0.6047 - accuracy: 0.66 - ETA: 0s - loss: 0.5990 - accuracy: 0.67 - ETA: 0s - loss: 0.6030 - accuracy: 0.66 - ETA: 0s - loss: 0.6031 - accuracy: 0.67 - ETA: 0s - loss: 0.6028 - accuracy: 0.67 - ETA: 0s - loss: 0.6001 - accuracy: 0.67 - ETA: 0s - loss: 0.6037 - accuracy: 0.67 - ETA: 0s - loss: 0.6045 - accuracy: 0.67 - ETA: 0s - loss: 0.6052 - accuracy: 0.67 - ETA: 0s - loss: 0.6066 - accuracy: 0.67 - ETA: 0s - loss: 0.6087 - accuracy: 0.67 - 1s 371us/sample - loss: 0.6088 - accuracy: 0.6724 - val_loss: 0.5969 - val_accuracy: 0.7090\n",
      "Epoch 10/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6226 - accuracy: 0.67 - ETA: 0s - loss: 0.6234 - accuracy: 0.66 - ETA: 0s - loss: 0.6141 - accuracy: 0.67 - ETA: 0s - loss: 0.6102 - accuracy: 0.68 - ETA: 0s - loss: 0.6103 - accuracy: 0.68 - ETA: 0s - loss: 0.6141 - accuracy: 0.68 - ETA: 0s - loss: 0.6088 - accuracy: 0.68 - ETA: 1s - loss: 0.6119 - accuracy: 0.68 - ETA: 1s - loss: 0.6162 - accuracy: 0.67 - ETA: 0s - loss: 0.6135 - accuracy: 0.67 - ETA: 1s - loss: 0.6119 - accuracy: 0.67 - ETA: 0s - loss: 0.6107 - accuracy: 0.67 - ETA: 0s - loss: 0.6122 - accuracy: 0.67 - ETA: 0s - loss: 0.6090 - accuracy: 0.67 - ETA: 1s - loss: 0.6098 - accuracy: 0.67 - ETA: 0s - loss: 0.6115 - accuracy: 0.67 - ETA: 0s - loss: 0.6078 - accuracy: 0.67 - ETA: 0s - loss: 0.6065 - accuracy: 0.67 - ETA: 0s - loss: 0.6085 - accuracy: 0.67 - ETA: 0s - loss: 0.6085 - accuracy: 0.67 - ETA: 0s - loss: 0.6016 - accuracy: 0.68 - ETA: 0s - loss: 0.6079 - accuracy: 0.67 - ETA: 0s - loss: 0.6064 - accuracy: 0.67 - ETA: 0s - loss: 0.6044 - accuracy: 0.67 - ETA: 0s - loss: 0.6069 - accuracy: 0.67 - ETA: 0s - loss: 0.6074 - accuracy: 0.67 - 2s 505us/sample - loss: 0.6073 - accuracy: 0.6724 - val_loss: 0.5932 - val_accuracy: 0.7090\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_1_units: 238</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_2_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_3_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_4_units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-input_units: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-no. Of Hidden Layers: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7089820504188538</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3336 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "3336/3336 [==============================] - ETA: 3:03 - loss: 0.6435 - accuracy: 0.75 - ETA: 25s - loss: 0.6570 - accuracy: 0.6607 - ETA: 13s - loss: 0.6384 - accuracy: 0.687 - ETA: 8s - loss: 0.6541 - accuracy: 0.672 - ETA: 6s - loss: 0.6533 - accuracy: 0.66 - ETA: 4s - loss: 0.6492 - accuracy: 0.66 - ETA: 3s - loss: 0.6464 - accuracy: 0.67 - ETA: 3s - loss: 0.6454 - accuracy: 0.67 - ETA: 2s - loss: 0.6429 - accuracy: 0.67 - ETA: 1s - loss: 0.6431 - accuracy: 0.67 - ETA: 1s - loss: 0.6421 - accuracy: 0.67 - ETA: 1s - loss: 0.6405 - accuracy: 0.67 - ETA: 0s - loss: 0.6396 - accuracy: 0.67 - ETA: 0s - loss: 0.6401 - accuracy: 0.67 - ETA: 0s - loss: 0.6394 - accuracy: 0.67 - ETA: 0s - loss: 0.6412 - accuracy: 0.67 - ETA: 0s - loss: 0.6404 - accuracy: 0.67 - 3s 946us/sample - loss: 0.6405 - accuracy: 0.6709 - val_loss: 0.6226 - val_accuracy: 0.7090\n",
      "Epoch 2/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5986 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.68 - ETA: 0s - loss: 0.6324 - accuracy: 0.67 - ETA: 0s - loss: 0.6230 - accuracy: 0.68 - ETA: 0s - loss: 0.6210 - accuracy: 0.68 - ETA: 0s - loss: 0.6258 - accuracy: 0.68 - ETA: 0s - loss: 0.6329 - accuracy: 0.67 - ETA: 0s - loss: 0.6311 - accuracy: 0.67 - ETA: 0s - loss: 0.6305 - accuracy: 0.67 - ETA: 0s - loss: 0.6300 - accuracy: 0.67 - ETA: 0s - loss: 0.6293 - accuracy: 0.68 - ETA: 0s - loss: 0.6262 - accuracy: 0.68 - ETA: 0s - loss: 0.6299 - accuracy: 0.67 - ETA: 0s - loss: 0.6295 - accuracy: 0.67 - ETA: 0s - loss: 0.6310 - accuracy: 0.67 - ETA: 0s - loss: 0.6318 - accuracy: 0.67 - ETA: 0s - loss: 0.6341 - accuracy: 0.67 - ETA: 0s - loss: 0.6362 - accuracy: 0.66 - ETA: 0s - loss: 0.6386 - accuracy: 0.66 - ETA: 0s - loss: 0.6382 - accuracy: 0.66 - ETA: 0s - loss: 0.6368 - accuracy: 0.66 - ETA: 0s - loss: 0.6350 - accuracy: 0.66 - ETA: 0s - loss: 0.6332 - accuracy: 0.67 - 1s 444us/sample - loss: 0.6302 - accuracy: 0.6724 - val_loss: 0.5918 - val_accuracy: 0.7090\n",
      "Epoch 3/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6247 - accuracy: 0.68 - ETA: 0s - loss: 0.6891 - accuracy: 0.60 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6584 - accuracy: 0.63 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - ETA: 0s - loss: 0.6401 - accuracy: 0.65 - ETA: 0s - loss: 0.6379 - accuracy: 0.66 - ETA: 0s - loss: 0.6360 - accuracy: 0.66 - ETA: 0s - loss: 0.6280 - accuracy: 0.67 - ETA: 0s - loss: 0.6251 - accuracy: 0.67 - ETA: 0s - loss: 0.6263 - accuracy: 0.67 - ETA: 0s - loss: 0.6245 - accuracy: 0.67 - ETA: 0s - loss: 0.6255 - accuracy: 0.66 - ETA: 0s - loss: 0.6251 - accuracy: 0.67 - ETA: 0s - loss: 0.6206 - accuracy: 0.67 - ETA: 0s - loss: 0.6185 - accuracy: 0.67 - ETA: 0s - loss: 0.6204 - accuracy: 0.67 - 1s 328us/sample - loss: 0.6219 - accuracy: 0.6724 - val_loss: 0.6017 - val_accuracy: 0.7090\n",
      "Epoch 4/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5700 - accuracy: 0.71 - ETA: 0s - loss: 0.5946 - accuracy: 0.69 - ETA: 0s - loss: 0.6095 - accuracy: 0.67 - ETA: 0s - loss: 0.6142 - accuracy: 0.67 - ETA: 0s - loss: 0.6157 - accuracy: 0.67 - ETA: 0s - loss: 0.6099 - accuracy: 0.67 - ETA: 0s - loss: 0.6107 - accuracy: 0.66 - ETA: 0s - loss: 0.6097 - accuracy: 0.66 - ETA: 0s - loss: 0.6058 - accuracy: 0.67 - ETA: 0s - loss: 0.6075 - accuracy: 0.66 - ETA: 0s - loss: 0.6115 - accuracy: 0.66 - ETA: 0s - loss: 0.6144 - accuracy: 0.66 - ETA: 0s - loss: 0.6157 - accuracy: 0.66 - ETA: 0s - loss: 0.6121 - accuracy: 0.67 - ETA: 0s - loss: 0.6112 - accuracy: 0.67 - ETA: 0s - loss: 0.6086 - accuracy: 0.67 - ETA: 0s - loss: 0.6127 - accuracy: 0.67 - ETA: 0s - loss: 0.6124 - accuracy: 0.67 - 1s 329us/sample - loss: 0.6137 - accuracy: 0.6724 - val_loss: 0.6045 - val_accuracy: 0.7090\n",
      "Epoch 5/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5965 - accuracy: 0.65 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.65 - ETA: 0s - loss: 0.6327 - accuracy: 0.65 - ETA: 0s - loss: 0.6275 - accuracy: 0.65 - ETA: 0s - loss: 0.6236 - accuracy: 0.66 - ETA: 0s - loss: 0.6241 - accuracy: 0.66 - ETA: 0s - loss: 0.6211 - accuracy: 0.66 - ETA: 0s - loss: 0.6158 - accuracy: 0.67 - ETA: 0s - loss: 0.6138 - accuracy: 0.67 - ETA: 0s - loss: 0.6151 - accuracy: 0.67 - ETA: 0s - loss: 0.6139 - accuracy: 0.67 - ETA: 0s - loss: 0.6107 - accuracy: 0.67 - ETA: 0s - loss: 0.6120 - accuracy: 0.67 - 1s 323us/sample - loss: 0.6119 - accuracy: 0.6733 - val_loss: 0.5861 - val_accuracy: 0.7090\n",
      "Epoch 6/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5627 - accuracy: 0.65 - ETA: 0s - loss: 0.5752 - accuracy: 0.67 - ETA: 0s - loss: 0.5843 - accuracy: 0.67 - ETA: 0s - loss: 0.5731 - accuracy: 0.68 - ETA: 0s - loss: 0.5927 - accuracy: 0.67 - ETA: 0s - loss: 0.5888 - accuracy: 0.67 - ETA: 0s - loss: 0.6034 - accuracy: 0.66 - ETA: 0s - loss: 0.6024 - accuracy: 0.67 - ETA: 0s - loss: 0.6027 - accuracy: 0.67 - ETA: 0s - loss: 0.6040 - accuracy: 0.67 - ETA: 0s - loss: 0.6013 - accuracy: 0.67 - ETA: 0s - loss: 0.6004 - accuracy: 0.67 - ETA: 0s - loss: 0.6022 - accuracy: 0.67 - ETA: 0s - loss: 0.6018 - accuracy: 0.67 - ETA: 0s - loss: 0.6025 - accuracy: 0.67 - ETA: 0s - loss: 0.6050 - accuracy: 0.66 - ETA: 0s - loss: 0.6058 - accuracy: 0.66 - ETA: 0s - loss: 0.6051 - accuracy: 0.67 - 1s 322us/sample - loss: 0.6041 - accuracy: 0.6733 - val_loss: 0.5818 - val_accuracy: 0.7090\n",
      "Epoch 7/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.7190 - accuracy: 0.53 - ETA: 0s - loss: 0.6078 - accuracy: 0.67 - ETA: 0s - loss: 0.6277 - accuracy: 0.66 - ETA: 0s - loss: 0.6193 - accuracy: 0.66 - ETA: 0s - loss: 0.6112 - accuracy: 0.67 - ETA: 0s - loss: 0.6048 - accuracy: 0.67 - ETA: 0s - loss: 0.5993 - accuracy: 0.68 - ETA: 0s - loss: 0.5974 - accuracy: 0.68 - ETA: 0s - loss: 0.5917 - accuracy: 0.68 - ETA: 0s - loss: 0.5976 - accuracy: 0.68 - ETA: 0s - loss: 0.5965 - accuracy: 0.68 - ETA: 0s - loss: 0.5967 - accuracy: 0.68 - ETA: 0s - loss: 0.5990 - accuracy: 0.68 - ETA: 0s - loss: 0.6033 - accuracy: 0.67 - ETA: 0s - loss: 0.6027 - accuracy: 0.67 - ETA: 0s - loss: 0.6041 - accuracy: 0.67 - ETA: 0s - loss: 0.6048 - accuracy: 0.67 - ETA: 0s - loss: 0.6055 - accuracy: 0.67 - 1s 347us/sample - loss: 0.6063 - accuracy: 0.6733 - val_loss: 0.5957 - val_accuracy: 0.7126\n",
      "Epoch 8/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6043 - accuracy: 0.65 - ETA: 0s - loss: 0.6165 - accuracy: 0.63 - ETA: 0s - loss: 0.6060 - accuracy: 0.64 - ETA: 0s - loss: 0.6166 - accuracy: 0.64 - ETA: 0s - loss: 0.6132 - accuracy: 0.65 - ETA: 0s - loss: 0.6048 - accuracy: 0.66 - ETA: 0s - loss: 0.6056 - accuracy: 0.66 - ETA: 0s - loss: 0.6085 - accuracy: 0.66 - ETA: 0s - loss: 0.6097 - accuracy: 0.66 - ETA: 0s - loss: 0.6070 - accuracy: 0.66 - ETA: 0s - loss: 0.6045 - accuracy: 0.67 - ETA: 0s - loss: 0.6073 - accuracy: 0.66 - ETA: 0s - loss: 0.6071 - accuracy: 0.66 - ETA: 0s - loss: 0.6060 - accuracy: 0.67 - ETA: 0s - loss: 0.6033 - accuracy: 0.67 - ETA: 0s - loss: 0.6011 - accuracy: 0.67 - ETA: 0s - loss: 0.6027 - accuracy: 0.67 - ETA: 0s - loss: 0.6025 - accuracy: 0.67 - 1s 358us/sample - loss: 0.6009 - accuracy: 0.6748 - val_loss: 0.5956 - val_accuracy: 0.7138\n",
      "Epoch 9/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.5910 - accuracy: 0.56 - ETA: 0s - loss: 0.6226 - accuracy: 0.65 - ETA: 0s - loss: 0.5973 - accuracy: 0.67 - ETA: 0s - loss: 0.5947 - accuracy: 0.66 - ETA: 0s - loss: 0.5987 - accuracy: 0.65 - ETA: 0s - loss: 0.5949 - accuracy: 0.66 - ETA: 0s - loss: 0.5992 - accuracy: 0.66 - ETA: 0s - loss: 0.5959 - accuracy: 0.66 - ETA: 0s - loss: 0.5947 - accuracy: 0.67 - ETA: 0s - loss: 0.5968 - accuracy: 0.67 - ETA: 0s - loss: 0.5956 - accuracy: 0.67 - ETA: 0s - loss: 0.5929 - accuracy: 0.68 - ETA: 0s - loss: 0.5893 - accuracy: 0.68 - ETA: 0s - loss: 0.5969 - accuracy: 0.68 - ETA: 0s - loss: 0.5960 - accuracy: 0.68 - ETA: 0s - loss: 0.5973 - accuracy: 0.68 - ETA: 0s - loss: 0.5986 - accuracy: 0.68 - ETA: 0s - loss: 0.6001 - accuracy: 0.67 - 1s 342us/sample - loss: 0.5999 - accuracy: 0.6781 - val_loss: 0.5946 - val_accuracy: 0.7066\n",
      "Epoch 10/10\n",
      "3336/3336 [==============================] - ETA: 1s - loss: 0.6603 - accuracy: 0.59 - ETA: 0s - loss: 0.6085 - accuracy: 0.66 - ETA: 0s - loss: 0.6105 - accuracy: 0.66 - ETA: 0s - loss: 0.5953 - accuracy: 0.68 - ETA: 0s - loss: 0.5974 - accuracy: 0.68 - ETA: 0s - loss: 0.5984 - accuracy: 0.68 - ETA: 0s - loss: 0.6030 - accuracy: 0.67 - ETA: 0s - loss: 0.5994 - accuracy: 0.68 - ETA: 0s - loss: 0.5991 - accuracy: 0.68 - ETA: 0s - loss: 0.5995 - accuracy: 0.68 - ETA: 0s - loss: 0.6007 - accuracy: 0.68 - ETA: 0s - loss: 0.6030 - accuracy: 0.67 - ETA: 0s - loss: 0.5948 - accuracy: 0.68 - ETA: 0s - loss: 0.5953 - accuracy: 0.68 - ETA: 0s - loss: 0.5986 - accuracy: 0.68 - ETA: 0s - loss: 0.5957 - accuracy: 0.68 - ETA: 0s - loss: 0.5977 - accuracy: 0.67 - ETA: 0s - loss: 0.5967 - accuracy: 0.67 - 1s 326us/sample - loss: 0.5972 - accuracy: 0.6763 - val_loss: 0.5896 - val_accuracy: 0.6958\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_1_units: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_2_units: 126</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Hidden_layer_3_units: 142</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-Hidden_layer_4_units: 174</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-input_units: 62</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-no. Of Hidden Layers: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7137724757194519</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             epochs = EPOCHS,\n",
    "#             batch_size = 64,\n",
    "             callbacks=[early_stop],\n",
    "             validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in 1579292506/untitled_project</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_accuracy', direction='max') Score: 0.714970052242279</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_accuracy', direction='max') Score: 0.7089820504188538</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'input_units': 158, 'no. Of Hidden Layers': 2, 'Hidden_layer_1_units': 62, 'Hidden_layer_2_units': 30}\n"
     ]
    }
   ],
   "source": [
    "print(tuner.results_summary()) # top 10 trials\n",
    "print(tuner.get_best_hyperparameters()[0].values) #values of best hyper-parameters\n",
    "#print(tuner.get_best_models()[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
